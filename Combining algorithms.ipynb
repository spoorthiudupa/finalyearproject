{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for without fusion\n",
    "df = pd.read_excel(\"cnn.xlsx\",sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for with fusion\n",
    "df = pd.read_excel(\"cnn_abcd.xlsx\",sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([       1,        2,        3,        4,        5,        6,        7,\n",
       "              8,        9,       10,       11,       12,       13,       14,\n",
       "             15,       16,       17,       18,       19,       20,       21,\n",
       "             22,       23,       24,       25,       26,       27,       28,\n",
       "             29,       30,       31,       32,       33,       34,       35,\n",
       "             36,       37,       38,       39,       40,       41,       42,\n",
       "             43,       44,       45,       46,       47,       48,       49,\n",
       "             50, 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = df.columns\n",
    "\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "622    1\n",
      "623    1\n",
      "624    1\n",
      "625    1\n",
      "626    1\n",
      "Name: target, Length: 627, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['target'], axis=1)\n",
    "\n",
    "y = df['target']\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((438, 50), (189, 50))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n",
    "\n",
    "# clf_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# grad_boost_clf = GradientBoostingClassifier(n_estimators=1500, random_state=42)\n",
    "# grad_boost_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt=clf_gini.predict_proba(X_test)\n",
    "#gb=grad_boost_clf.predict_proba(X_test)\n",
    "#print(svm)\n",
    "#print(lr)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spoor\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:16:22] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[7.16852244e-02 1.12831478e+00]\n",
      " [6.21020827e-02 1.13789792e+00]\n",
      " [7.02721224e-02 1.12972788e+00]\n",
      " [1.02324905e+00 1.76750956e-01]\n",
      " [1.12960746e+00 7.03925363e-02]\n",
      " [1.11181211e+00 8.81878968e-02]\n",
      " [1.11942906e+00 8.05709399e-02]\n",
      " [8.01519087e-02 1.11984810e+00]\n",
      " [7.05416898e-01 4.94583105e-01]\n",
      " [1.12963397e+00 7.03660332e-02]\n",
      " [2.34979818e-01 9.65020182e-01]\n",
      " [2.34429799e-02 1.17655702e+00]\n",
      " [4.38194265e-01 7.61805738e-01]\n",
      " [4.84554184e-02 1.15154458e+00]\n",
      " [1.08656271e-01 1.09134373e+00]\n",
      " [1.90871549e-02 1.18091284e+00]\n",
      " [1.96552052e-01 1.00344796e+00]\n",
      " [3.81470554e-02 1.16185294e+00]\n",
      " [3.28115077e-01 8.71884926e-01]\n",
      " [1.23086305e-01 1.07691370e+00]\n",
      " [1.07344514e+00 1.26554860e-01]\n",
      " [6.14926738e-02 1.13850733e+00]\n",
      " [1.10548706e+00 9.45129471e-02]\n",
      " [1.32943250e-01 1.06705675e+00]\n",
      " [2.19033014e-02 1.17809670e+00]\n",
      " [3.91437295e-02 1.16085627e+00]\n",
      " [1.17756796e+00 2.24320388e-02]\n",
      " [1.93539961e-01 1.00646004e+00]\n",
      " [1.10788891e+00 9.21110874e-02]\n",
      " [1.10879317e+00 9.12068416e-02]\n",
      " [1.04724113e-01 1.09527588e+00]\n",
      " [1.12337030e+00 7.66296891e-02]\n",
      " [8.27787083e-01 3.72212928e-01]\n",
      " [1.29500868e-01 1.07049914e+00]\n",
      " [1.13266327e+00 6.73367383e-02]\n",
      " [2.96854929e-01 9.03145072e-01]\n",
      " [1.06721852e+00 1.32781482e-01]\n",
      " [4.32632198e-01 7.67367801e-01]\n",
      " [3.88186255e-02 1.16118138e+00]\n",
      " [2.55607196e-02 1.17443928e+00]\n",
      " [9.00346977e-01 2.99653031e-01]\n",
      " [7.53704264e-01 4.46295738e-01]\n",
      " [1.09728052e+00 1.02719481e-01]\n",
      " [8.20567789e-01 3.79432218e-01]\n",
      " [1.10119364e+00 9.88063541e-02]\n",
      " [6.55623341e-01 5.44376662e-01]\n",
      " [6.95464370e-02 1.13045357e+00]\n",
      " [2.68946108e-02 1.17310539e+00]\n",
      " [3.56381288e-02 1.16436187e+00]\n",
      " [4.22354532e-01 7.77645469e-01]\n",
      " [8.61918878e-01 3.38081133e-01]\n",
      " [1.12818017e+00 7.18198303e-02]\n",
      " [1.15562446e+00 4.43755402e-02]\n",
      " [1.13377737e+00 6.62226391e-02]\n",
      " [9.29957818e-02 1.10700423e+00]\n",
      " [1.03253175e+00 1.67468260e-01]\n",
      " [2.82837527e-02 1.17171625e+00]\n",
      " [1.10692367e+00 9.30763270e-02]\n",
      " [3.89965583e-02 1.16100344e+00]\n",
      " [1.10220938e+00 9.77906121e-02]\n",
      " [1.39436939e-01 1.06056307e+00]\n",
      " [4.53761274e-01 7.46238729e-01]\n",
      " [1.19940367e+00 5.96334512e-04]\n",
      " [2.36221063e-02 1.17637789e+00]\n",
      " [1.10541014e+00 9.45898581e-02]\n",
      " [1.18768262e+00 1.23173861e-02]\n",
      " [1.14025629e+00 5.97437176e-02]\n",
      " [1.18714927e+00 1.28507327e-02]\n",
      " [1.12452678e+00 7.54732320e-02]\n",
      " [2.91106454e-02 1.17088936e+00]\n",
      " [3.04268092e-02 1.16957320e+00]\n",
      " [1.09872911e+00 1.01270902e-01]\n",
      " [2.55932965e-02 1.17440671e+00]\n",
      " [1.16727383e+00 3.27261757e-02]\n",
      " [1.14825113e+00 5.17488809e-02]\n",
      " [1.17817470e+00 2.18252962e-02]\n",
      " [1.06588783e+00 1.34112169e-01]\n",
      " [3.09726482e-02 1.16902736e+00]\n",
      " [1.16173078e+00 3.82692223e-02]\n",
      " [1.76215450e-02 1.18237846e+00]\n",
      " [1.13171691e-01 1.08682832e+00]\n",
      " [1.12330019e+00 7.66998149e-02]\n",
      " [3.54228435e-02 1.16457716e+00]\n",
      " [1.10412497e+00 9.58750309e-02]\n",
      " [2.47982709e-01 9.52017297e-01]\n",
      " [8.65908017e-01 3.34091993e-01]\n",
      " [2.36893529e-02 1.17631066e+00]\n",
      " [1.04752727e+00 1.52472737e-01]\n",
      " [2.44077982e-02 1.17559220e+00]\n",
      " [1.13691984e+00 6.30801684e-02]\n",
      " [1.07348716e+00 1.26512852e-01]\n",
      " [1.17169865e+00 2.83013480e-02]\n",
      " [1.08627011e+00 1.13729903e-01]\n",
      " [7.61158584e-01 4.38841411e-01]\n",
      " [7.61531916e-01 4.38468089e-01]\n",
      " [1.97997816e-02 1.18020023e+00]\n",
      " [1.15212501e+00 4.78749968e-02]\n",
      " [1.36065900e-01 1.06393411e+00]\n",
      " [1.77229858e-01 1.02277014e+00]\n",
      " [1.12495167e+00 7.50483278e-02]\n",
      " [1.07400083e+00 1.25999165e-01]\n",
      " [4.91972933e-02 1.15080272e+00]\n",
      " [5.25225296e-01 6.74774707e-01]\n",
      " [1.13250684e+00 6.74931608e-02]\n",
      " [1.09669624e+00 1.03303769e-01]\n",
      " [2.14381504e-02 1.17856185e+00]\n",
      " [1.18755062e+00 1.24493811e-02]\n",
      " [7.03180174e-02 1.12968198e+00]\n",
      " [9.50317107e-02 1.10496830e+00]\n",
      " [1.16304704e+00 3.69529682e-02]\n",
      " [1.10885572e+00 9.11442786e-02]\n",
      " [1.16635112e+00 3.36488880e-02]\n",
      " [1.84363253e-02 1.18156368e+00]\n",
      " [1.05030877e+00 1.49691240e-01]\n",
      " [1.06579851e+00 1.34201488e-01]\n",
      " [1.54394224e-01 1.04560579e+00]\n",
      " [1.90245634e-02 1.18097545e+00]\n",
      " [5.47593682e-02 1.14524063e+00]\n",
      " [2.68724377e-01 9.31275620e-01]\n",
      " [1.10638922e+00 9.36107794e-02]\n",
      " [1.13773240e+00 6.22676125e-02]\n",
      " [5.34137356e-01 6.65862639e-01]\n",
      " [1.12774432e+00 7.22556897e-02]\n",
      " [5.63319169e-01 6.36680841e-01]\n",
      " [1.18875891e+00 1.12410876e-02]\n",
      " [8.79009928e-01 3.20990070e-01]\n",
      " [1.17217772e+00 2.78222896e-02]\n",
      " [1.07866752e+00 1.21332486e-01]\n",
      " [9.94567955e-02 1.10054320e+00]\n",
      " [2.27025067e-01 9.72974942e-01]\n",
      " [2.55947643e-02 1.17440524e+00]\n",
      " [1.13780239e+00 6.21976132e-02]\n",
      " [1.87250441e-02 1.18127495e+00]\n",
      " [1.17606040e+00 2.39396066e-02]\n",
      " [1.18564889e+00 1.43511077e-02]\n",
      " [1.53156733e-01 1.04684327e+00]\n",
      " [1.07930122e+00 1.20698785e-01]\n",
      " [3.61049666e-01 8.38950337e-01]\n",
      " [8.55767073e-01 3.44232925e-01]\n",
      " [2.56683126e-02 1.17433169e+00]\n",
      " [1.87234078e-02 1.18127659e+00]\n",
      " [1.15715296e+00 4.28470427e-02]\n",
      " [1.13316705e+00 6.68329548e-02]\n",
      " [2.30119047e-02 1.17698810e+00]\n",
      " [1.14318387e+00 5.68161278e-02]\n",
      " [1.16271846e+00 3.72815425e-02]\n",
      " [5.00484488e-01 6.99515521e-01]\n",
      " [2.04766358e-02 1.17952337e+00]\n",
      " [3.61565359e-01 8.38434643e-01]\n",
      " [2.26072456e-02 1.17739275e+00]\n",
      " [1.08744219e+00 1.12557821e-01]\n",
      " [7.44334952e-01 4.55665056e-01]\n",
      " [1.74954002e-02 1.18250460e+00]\n",
      " [3.35133454e-02 1.16648665e+00]\n",
      " [1.17466095e+00 2.53390564e-02]\n",
      " [2.17683770e-02 1.17823162e+00]\n",
      " [6.27035109e-01 5.72964894e-01]\n",
      " [1.05741410e+00 1.42585904e-01]\n",
      " [3.62947820e-02 1.16370522e+00]\n",
      " [1.09317442e+00 1.06825581e-01]\n",
      " [1.19924750e+00 7.52504477e-04]\n",
      " [5.55560877e-02 1.14444392e+00]\n",
      " [1.46081444e-01 1.05391856e+00]\n",
      " [9.24902476e-01 2.75097533e-01]\n",
      " [1.14028324e+00 5.97167556e-02]\n",
      " [2.08026692e-02 1.17919733e+00]\n",
      " [2.21229892e-02 1.17787702e+00]\n",
      " [1.07765212e+00 1.22347879e-01]\n",
      " [1.08317222e+00 1.16827786e-01]\n",
      " [1.78846357e-02 1.18211537e+00]\n",
      " [4.71176656e-02 1.15288233e+00]\n",
      " [1.12065284e+00 7.93471730e-02]\n",
      " [2.25797881e-01 9.74202128e-01]\n",
      " [9.52804836e-02 1.10471953e+00]\n",
      " [1.16639274e+00 3.36072659e-02]\n",
      " [1.74550013e-02 1.18254501e+00]\n",
      " [5.97708689e-01 6.02291314e-01]\n",
      " [3.06840670e-02 1.16931593e+00]\n",
      " [6.59726964e-02 1.13402730e+00]\n",
      " [1.35243597e-01 1.06475640e+00]\n",
      " [1.40422802e-01 1.05957720e+00]\n",
      " [2.58229144e-01 9.41770859e-01]\n",
      " [2.31458573e-02 1.17685415e+00]\n",
      " [6.35411936e-02 1.13645881e+00]\n",
      " [4.81077957e-02 1.15189220e+00]\n",
      " [2.53842811e-02 1.17461572e+00]\n",
      " [1.74146153e-01 1.02585384e+00]\n",
      " [1.18728899e+00 1.27110104e-02]\n",
      " [2.06047912e-02 1.17939521e+00]]\n"
     ]
    }
   ],
   "source": [
    "svc=SVC(kernel='linear',C=0.5,probability=True)\n",
    "svc.fit(X_train,y_train)\n",
    "svm=svc.predict_proba(X_test)\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "logreg.fit(X_train,y_train)\n",
    "lr=logreg.predict_proba(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n",
    "clf_gini.fit(X_train,y_train)\n",
    "dt=clf_gini.predict_proba(X_test)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "grad_boost_clf = GradientBoostingClassifier(n_estimators=1500, random_state=42)\n",
    "grad_boost_clf.fit(X_train,y_train)\n",
    "gb=grad_boost_clf.predict_proba(X_test)\n",
    "\n",
    "import xgboost as xgb\n",
    "xg = xgb.XGBClassifier()\n",
    "xg.fit(X_train,y_train)\n",
    "x=xg.predict_proba(X_test)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bc = BaggingClassifier(base_estimator=tree, n_estimators=1500, random_state=42)\n",
    "bc.fit(X_train,y_train)\n",
    "b=bc.predict_proba(X_test)\n",
    "\n",
    "\n",
    "ensemble_probs=(0.2*svm)+(0.2*lr)+(0.2*dt)+(0.2*gb)+(0.2*x)+(0.2*b)\n",
    "print(ensemble_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0\n",
      " 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0\n",
      " 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
      " 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1\n",
      " 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "preds=np.argmax(ensemble_probs,axis=1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc=accuracy_score(y_test,preds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spoor\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\spoor\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:16:40] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.9206349206349206\n",
      "Precision: 0.8888888888888888\n",
      "Recall: 0.9491525423728814\n",
      "F1-score: 0.9180327868852458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize SVM and logistic regression models\n",
    "svc=SVC(kernel='linear',C=0.5,probability=True)\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "grad_boost_clf = GradientBoostingClassifier(n_estimators=1500, random_state=42)\n",
    "\n",
    "import xgboost as xgb\n",
    "xg = xgb.XGBClassifier()\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bc = BaggingClassifier(base_estimator=tree, n_estimators=1500, random_state=42)\n",
    "\n",
    "\n",
    "# Create a voting classifier using the SVM and logistic regression models\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('svc', svc), \n",
    "    ('lr',logreg),\n",
    "    ('clf',clf_gini),\n",
    "    ('gb',grad_boost_clf),\n",
    "    ('xg',xg),\n",
    "    ('bc', bc)\n",
    "], voting='soft')\n",
    "\n",
    "# Train the voting classifier on the training data\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data using the voting classifier\n",
    "y_pred = voting.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on the validation data\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the testing data using the voting classifier\n",
    "y_pred_test = voting.predict(X_test)\n",
    "\n",
    "# Calculate precision, recall, and F1-score on the testing data\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spoor\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\spoor\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:16:54] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spoor\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\spoor\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:17:09] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:17:09] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:17:09] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:17:09] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:17:09] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.9285714285714286\n",
      "Precision: 0.9032258064516129\n",
      "Recall: 0.9491525423728814\n",
      "F1-score: 0.9256198347107438\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training set further into two parts: training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize the base models\n",
    "estimators = [\n",
    "             ('svm', SVC(kernel='linear',C=0.5,probability=True)),\n",
    "             ('lr', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "             ('dt',DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)),\n",
    "             ('gb',GradientBoostingClassifier(n_estimators=1500, random_state=42)),\n",
    "             ('xg',xgb.XGBClassifier()),\n",
    "             ('bc',BaggingClassifier(base_estimator=tree, n_estimators=1500, random_state=42))\n",
    "             ]\n",
    "\n",
    "# Initialize the meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Initialize the stacking classifier\n",
    "stacking = StackingClassifier(estimators=estimators, final_estimator=meta_model)\n",
    "\n",
    "# Train the stacking classifier on the training set\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set using the base models\n",
    "X_val_base_pred = np.column_stack([model.predict(X_val) for model in stacking.estimators_])\n",
    "\n",
    "# Make predictions on the validation set using the meta-model\n",
    "y_val_pred = stacking.final_estimator_.predict(X_val_base_pred)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the validation set\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred)\n",
    "recall = recall_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "# print('Precision:', precision)\n",
    "# print('Recall:', recall)\n",
    "# print('F1-score:', f1)\n",
    "\n",
    "# Make predictions on the test set using the base models\n",
    "X_test_base_pred = np.column_stack([model.predict(X_test) for model in stacking.estimators_])\n",
    "\n",
    "# Make predictions on the test set using the meta-model\n",
    "y_test_pred = stacking.final_estimator_.predict(X_test_base_pred)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
